{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "transf DeepLOB.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLucMg3r_ZzG"
      },
      "source": [
        "from os import PathLike\n",
        "from os.path import join as joinpath\n",
        "from time import time\n",
        "from typing import Union, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import (\n",
        "    Input,\n",
        "    Conv2D,\n",
        "    MaxPooling2D,\n",
        "    LeakyReLU,\n",
        "    LSTM,\n",
        "    Dense,\n",
        "    Reshape,\n",
        "    concatenate\n",
        ")\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "LOOKUP_WINDOW_SIZE = 100\n",
        "EPOCHS = 200"
      ],
      "id": "xLucMg3r_ZzG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k26aFTEJDFuT",
        "outputId": "5fc2e126-5c88-4bfe-dcc2-9fa3ee10d84c"
      },
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "    # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "    # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)"
      ],
      "id": "k26aFTEJDFuT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 Physical GPUs, 1 Logical GPUs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pg1s1vuAKu4",
        "outputId": "f682b51a-c8c4-4e79-c593-61c0377abceb"
      },
      "source": [
        "import datetime\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "tb_log  = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train_path = 'Train_Dst_NoAuction_ZScore_CF_9.txt'\n",
        "test_path  = 'Test_Dst_NoAuction_ZScore_CF_9.txt'\n",
        "model_path = f'model_{int(time())}.h5'\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    LOB_TRADING_DIR = '/content/drive/MyDrive/Colab Notebooks/LOB Trading/'\n",
        "    MODEL_TRADING_DIR = '/content/drive/MyDrive/Colab Notebooks/'\n",
        "    train_path = joinpath(LOB_TRADING_DIR, train_path)\n",
        "    test_path  = joinpath(LOB_TRADING_DIR, test_path)\n",
        "    model_path = joinpath(MODEL_TRADING_DIR, model_path)\n",
        "    tb_path = joinpath(MODEL_TRADING_DIR, tb_log)\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_path, histogram_freq=1)\n",
        "except ModuleNotFoundError:\n",
        "    pass"
      ],
      "id": "4pg1s1vuAKu4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSeD7OuK_ZzI"
      },
      "source": [
        "def read_data(path: Union[str, PathLike]) -> pd.DataFrame:\n",
        "    with open(path, 'r') as file:\n",
        "        data = [\n",
        "            tuple(map(float, line.split()))\n",
        "            for line in tqdm(file)\n",
        "        ]\n",
        "    data = pd.DataFrame(data)\n",
        "    #   \n",
        "    data = data.loc[list(data.index[:40]) + list(data.index[-5:])]\n",
        "    data = data.T\n",
        "\n",
        "    # Same notation as in http://dx.doi.org/10.1002/for.2543\n",
        "    target_names = [f'l{i:02}' for i in (1, 2, 3, 5, 10)]\n",
        "\n",
        "    colnames = [\n",
        "        f'{feature}{i}'\n",
        "        for i in range(10)\n",
        "        for feature in ('Pa', 'Va', 'Pb', 'Vb')\n",
        "    ]\n",
        "    colnames += target_names\n",
        "\n",
        "    data.columns = colnames\n",
        "    for tg in target_names:\n",
        "        data[tg] = data[tg].astype(int)\n",
        "        data[tg] -= 1  # Needed for tensorflow.keras.utils.to_categorical\n",
        "    return data\n",
        "\n",
        "\n",
        "def get_ts_generator(data: pd.DataFrame,\n",
        "                     target_name: str,\n",
        "                     num_features: int = 40) -> TimeseriesGenerator:\n",
        "\n",
        "    X = data.iloc[:, :num_features].values\n",
        "    X = X[..., None]  # needed for CNN input\n",
        "    y = to_categorical(data[target_name].shift(fill_value=0))\n",
        "    # Shift forward by 1, since TimeSeriesGenerator maps every X[i] with y[i+1], but not with y[i]\n",
        "    return TimeseriesGenerator(X, y, LOOKUP_WINDOW_SIZE, shuffle=True, batch_size=BATCH_SIZE)"
      ],
      "id": "eSeD7OuK_ZzI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "aQd1zXCh_ZzJ",
        "outputId": "56a53395-bbaf-4163-acc2-26cc670b8678"
      },
      "source": [
        "train_data = read_data(train_path)\n",
        "test_data  = read_data(test_path)\n",
        "train_data"
      ],
      "id": "aQd1zXCh_ZzJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "149it [00:18,  8.10it/s]\n",
            "149it [00:02, 56.40it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pa0</th>\n",
              "      <th>Va0</th>\n",
              "      <th>Pb0</th>\n",
              "      <th>Vb0</th>\n",
              "      <th>Pa1</th>\n",
              "      <th>Va1</th>\n",
              "      <th>Pb1</th>\n",
              "      <th>Vb1</th>\n",
              "      <th>Pa2</th>\n",
              "      <th>Va2</th>\n",
              "      <th>Pb2</th>\n",
              "      <th>Vb2</th>\n",
              "      <th>Pa3</th>\n",
              "      <th>Va3</th>\n",
              "      <th>Pb3</th>\n",
              "      <th>Vb3</th>\n",
              "      <th>Pa4</th>\n",
              "      <th>Va4</th>\n",
              "      <th>Pb4</th>\n",
              "      <th>Vb4</th>\n",
              "      <th>Pa5</th>\n",
              "      <th>Va5</th>\n",
              "      <th>Pb5</th>\n",
              "      <th>Vb5</th>\n",
              "      <th>Pa6</th>\n",
              "      <th>Va6</th>\n",
              "      <th>Pb6</th>\n",
              "      <th>Vb6</th>\n",
              "      <th>Pa7</th>\n",
              "      <th>Va7</th>\n",
              "      <th>Pb7</th>\n",
              "      <th>Vb7</th>\n",
              "      <th>Pa8</th>\n",
              "      <th>Va8</th>\n",
              "      <th>Pb8</th>\n",
              "      <th>Vb8</th>\n",
              "      <th>Pa9</th>\n",
              "      <th>Va9</th>\n",
              "      <th>Pb9</th>\n",
              "      <th>Vb9</th>\n",
              "      <th>l01</th>\n",
              "      <th>l02</th>\n",
              "      <th>l03</th>\n",
              "      <th>l05</th>\n",
              "      <th>l10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.318116</td>\n",
              "      <td>-0.564619</td>\n",
              "      <td>0.313539</td>\n",
              "      <td>-0.551889</td>\n",
              "      <td>0.319726</td>\n",
              "      <td>-0.731228</td>\n",
              "      <td>0.312891</td>\n",
              "      <td>-0.425448</td>\n",
              "      <td>0.319404</td>\n",
              "      <td>-0.844157</td>\n",
              "      <td>0.312254</td>\n",
              "      <td>-0.571766</td>\n",
              "      <td>0.319138</td>\n",
              "      <td>-0.769659</td>\n",
              "      <td>0.311546</td>\n",
              "      <td>-0.619174</td>\n",
              "      <td>0.318881</td>\n",
              "      <td>-0.956744</td>\n",
              "      <td>0.311819</td>\n",
              "      <td>-0.885627</td>\n",
              "      <td>0.319478</td>\n",
              "      <td>-0.492876</td>\n",
              "      <td>0.309257</td>\n",
              "      <td>-0.807128</td>\n",
              "      <td>0.319920</td>\n",
              "      <td>-0.703866</td>\n",
              "      <td>0.308812</td>\n",
              "      <td>-0.742000</td>\n",
              "      <td>0.319202</td>\n",
              "      <td>-0.357451</td>\n",
              "      <td>0.308565</td>\n",
              "      <td>-0.624978</td>\n",
              "      <td>0.320242</td>\n",
              "      <td>-0.483313</td>\n",
              "      <td>0.307526</td>\n",
              "      <td>-0.533218</td>\n",
              "      <td>0.322061</td>\n",
              "      <td>-0.427763</td>\n",
              "      <td>0.300703</td>\n",
              "      <td>-0.480828</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.318116</td>\n",
              "      <td>-0.662079</td>\n",
              "      <td>0.313539</td>\n",
              "      <td>-0.551889</td>\n",
              "      <td>0.320706</td>\n",
              "      <td>-0.751891</td>\n",
              "      <td>0.312891</td>\n",
              "      <td>-0.425448</td>\n",
              "      <td>0.320383</td>\n",
              "      <td>-0.854876</td>\n",
              "      <td>0.312254</td>\n",
              "      <td>-0.571766</td>\n",
              "      <td>0.320117</td>\n",
              "      <td>-0.764619</td>\n",
              "      <td>0.312528</td>\n",
              "      <td>-0.802672</td>\n",
              "      <td>0.322798</td>\n",
              "      <td>-0.957195</td>\n",
              "      <td>0.312802</td>\n",
              "      <td>-0.749327</td>\n",
              "      <td>0.322415</td>\n",
              "      <td>-0.516778</td>\n",
              "      <td>0.313188</td>\n",
              "      <td>-0.781772</td>\n",
              "      <td>0.323834</td>\n",
              "      <td>-0.705590</td>\n",
              "      <td>0.310778</td>\n",
              "      <td>-0.761182</td>\n",
              "      <td>0.326050</td>\n",
              "      <td>-0.519047</td>\n",
              "      <td>0.310532</td>\n",
              "      <td>-0.621590</td>\n",
              "      <td>0.328063</td>\n",
              "      <td>-0.478852</td>\n",
              "      <td>0.310478</td>\n",
              "      <td>-0.530020</td>\n",
              "      <td>0.334763</td>\n",
              "      <td>-0.462173</td>\n",
              "      <td>0.309565</td>\n",
              "      <td>-0.481944</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.317136</td>\n",
              "      <td>-0.723163</td>\n",
              "      <td>0.313539</td>\n",
              "      <td>-0.551889</td>\n",
              "      <td>0.316787</td>\n",
              "      <td>-0.731228</td>\n",
              "      <td>0.312891</td>\n",
              "      <td>-0.425448</td>\n",
              "      <td>0.317445</td>\n",
              "      <td>-0.762942</td>\n",
              "      <td>0.312254</td>\n",
              "      <td>-0.571766</td>\n",
              "      <td>0.318159</td>\n",
              "      <td>-0.951519</td>\n",
              "      <td>0.312528</td>\n",
              "      <td>-0.802672</td>\n",
              "      <td>0.317902</td>\n",
              "      <td>-0.962596</td>\n",
              "      <td>0.312802</td>\n",
              "      <td>-0.749327</td>\n",
              "      <td>0.317521</td>\n",
              "      <td>-0.632462</td>\n",
              "      <td>0.313188</td>\n",
              "      <td>-0.781772</td>\n",
              "      <td>0.319920</td>\n",
              "      <td>-0.703866</td>\n",
              "      <td>0.310778</td>\n",
              "      <td>-0.761182</td>\n",
              "      <td>0.319202</td>\n",
              "      <td>-0.357451</td>\n",
              "      <td>0.310532</td>\n",
              "      <td>-0.621590</td>\n",
              "      <td>0.320242</td>\n",
              "      <td>-0.483313</td>\n",
              "      <td>0.310478</td>\n",
              "      <td>-0.530020</td>\n",
              "      <td>0.322061</td>\n",
              "      <td>-0.427763</td>\n",
              "      <td>0.309565</td>\n",
              "      <td>-0.481944</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.317136</td>\n",
              "      <td>-0.585895</td>\n",
              "      <td>0.313539</td>\n",
              "      <td>-0.551889</td>\n",
              "      <td>0.318747</td>\n",
              "      <td>-0.307628</td>\n",
              "      <td>0.312891</td>\n",
              "      <td>-0.425448</td>\n",
              "      <td>0.319404</td>\n",
              "      <td>-0.561348</td>\n",
              "      <td>0.312254</td>\n",
              "      <td>-0.571766</td>\n",
              "      <td>0.319138</td>\n",
              "      <td>-0.769659</td>\n",
              "      <td>0.311546</td>\n",
              "      <td>-0.619174</td>\n",
              "      <td>0.318881</td>\n",
              "      <td>-0.956744</td>\n",
              "      <td>0.311819</td>\n",
              "      <td>-0.885627</td>\n",
              "      <td>0.321436</td>\n",
              "      <td>-0.821286</td>\n",
              "      <td>0.309257</td>\n",
              "      <td>-0.807128</td>\n",
              "      <td>0.323834</td>\n",
              "      <td>-0.705590</td>\n",
              "      <td>0.308812</td>\n",
              "      <td>-0.742000</td>\n",
              "      <td>0.326050</td>\n",
              "      <td>-0.519047</td>\n",
              "      <td>0.308565</td>\n",
              "      <td>-0.624978</td>\n",
              "      <td>0.328063</td>\n",
              "      <td>-0.478852</td>\n",
              "      <td>0.307526</td>\n",
              "      <td>-0.533218</td>\n",
              "      <td>0.334763</td>\n",
              "      <td>-0.462173</td>\n",
              "      <td>0.300703</td>\n",
              "      <td>-0.480828</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.317136</td>\n",
              "      <td>-0.585895</td>\n",
              "      <td>0.313539</td>\n",
              "      <td>-0.551889</td>\n",
              "      <td>0.318747</td>\n",
              "      <td>-0.307628</td>\n",
              "      <td>0.312891</td>\n",
              "      <td>-0.425448</td>\n",
              "      <td>0.319404</td>\n",
              "      <td>-0.561348</td>\n",
              "      <td>0.312254</td>\n",
              "      <td>-0.571766</td>\n",
              "      <td>0.319138</td>\n",
              "      <td>-0.922437</td>\n",
              "      <td>0.311546</td>\n",
              "      <td>-0.619174</td>\n",
              "      <td>0.318881</td>\n",
              "      <td>-0.779397</td>\n",
              "      <td>0.311819</td>\n",
              "      <td>-0.885627</td>\n",
              "      <td>0.321436</td>\n",
              "      <td>-0.821286</td>\n",
              "      <td>0.309257</td>\n",
              "      <td>-0.807128</td>\n",
              "      <td>0.323834</td>\n",
              "      <td>-0.705590</td>\n",
              "      <td>0.308812</td>\n",
              "      <td>-0.742000</td>\n",
              "      <td>0.326050</td>\n",
              "      <td>-0.519047</td>\n",
              "      <td>0.308565</td>\n",
              "      <td>-0.624978</td>\n",
              "      <td>0.328063</td>\n",
              "      <td>-0.478852</td>\n",
              "      <td>0.307526</td>\n",
              "      <td>-0.533218</td>\n",
              "      <td>0.334763</td>\n",
              "      <td>-0.462173</td>\n",
              "      <td>0.300703</td>\n",
              "      <td>-0.480828</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362395</th>\n",
              "      <td>1.328480</td>\n",
              "      <td>-0.608544</td>\n",
              "      <td>1.329517</td>\n",
              "      <td>-0.550603</td>\n",
              "      <td>1.327892</td>\n",
              "      <td>-0.795514</td>\n",
              "      <td>1.330085</td>\n",
              "      <td>-0.719126</td>\n",
              "      <td>1.328302</td>\n",
              "      <td>-0.870542</td>\n",
              "      <td>1.330707</td>\n",
              "      <td>-0.795843</td>\n",
              "      <td>1.330779</td>\n",
              "      <td>-0.890641</td>\n",
              "      <td>1.331192</td>\n",
              "      <td>-0.490725</td>\n",
              "      <td>1.330349</td>\n",
              "      <td>-0.641660</td>\n",
              "      <td>1.331664</td>\n",
              "      <td>-0.909209</td>\n",
              "      <td>1.329733</td>\n",
              "      <td>-0.239996</td>\n",
              "      <td>1.332312</td>\n",
              "      <td>-0.656713</td>\n",
              "      <td>1.328899</td>\n",
              "      <td>-0.412445</td>\n",
              "      <td>1.333185</td>\n",
              "      <td>-0.694266</td>\n",
              "      <td>1.327774</td>\n",
              "      <td>-0.522781</td>\n",
              "      <td>1.333393</td>\n",
              "      <td>-0.592603</td>\n",
              "      <td>1.326325</td>\n",
              "      <td>0.105092</td>\n",
              "      <td>1.334906</td>\n",
              "      <td>-0.478272</td>\n",
              "      <td>1.324595</td>\n",
              "      <td>-0.469732</td>\n",
              "      <td>1.335614</td>\n",
              "      <td>-0.369642</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362396</th>\n",
              "      <td>1.328480</td>\n",
              "      <td>-0.608544</td>\n",
              "      <td>1.329517</td>\n",
              "      <td>-0.550603</td>\n",
              "      <td>1.327892</td>\n",
              "      <td>-0.795514</td>\n",
              "      <td>1.330085</td>\n",
              "      <td>-0.611748</td>\n",
              "      <td>1.328302</td>\n",
              "      <td>-0.870542</td>\n",
              "      <td>1.330707</td>\n",
              "      <td>-0.795843</td>\n",
              "      <td>1.330779</td>\n",
              "      <td>-0.890641</td>\n",
              "      <td>1.331192</td>\n",
              "      <td>-0.490725</td>\n",
              "      <td>1.330349</td>\n",
              "      <td>-0.641660</td>\n",
              "      <td>1.331664</td>\n",
              "      <td>-0.729342</td>\n",
              "      <td>1.329733</td>\n",
              "      <td>-0.239996</td>\n",
              "      <td>1.331329</td>\n",
              "      <td>-0.742665</td>\n",
              "      <td>1.328899</td>\n",
              "      <td>-0.712057</td>\n",
              "      <td>1.331219</td>\n",
              "      <td>-0.707649</td>\n",
              "      <td>1.327774</td>\n",
              "      <td>-0.522781</td>\n",
              "      <td>1.332409</td>\n",
              "      <td>-0.557970</td>\n",
              "      <td>1.326325</td>\n",
              "      <td>0.250666</td>\n",
              "      <td>1.332938</td>\n",
              "      <td>-0.386987</td>\n",
              "      <td>1.324595</td>\n",
              "      <td>-0.469732</td>\n",
              "      <td>1.333645</td>\n",
              "      <td>-0.278103</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362397</th>\n",
              "      <td>1.328480</td>\n",
              "      <td>-0.608544</td>\n",
              "      <td>1.329517</td>\n",
              "      <td>-0.550603</td>\n",
              "      <td>1.327892</td>\n",
              "      <td>-0.795514</td>\n",
              "      <td>1.330085</td>\n",
              "      <td>-0.611748</td>\n",
              "      <td>1.328302</td>\n",
              "      <td>-0.870542</td>\n",
              "      <td>1.330707</td>\n",
              "      <td>-0.795843</td>\n",
              "      <td>1.330779</td>\n",
              "      <td>-0.890641</td>\n",
              "      <td>1.331192</td>\n",
              "      <td>-0.490725</td>\n",
              "      <td>1.330349</td>\n",
              "      <td>-0.979701</td>\n",
              "      <td>1.331664</td>\n",
              "      <td>-0.909209</td>\n",
              "      <td>1.329733</td>\n",
              "      <td>-0.239996</td>\n",
              "      <td>1.332312</td>\n",
              "      <td>-0.656713</td>\n",
              "      <td>1.328899</td>\n",
              "      <td>-0.712057</td>\n",
              "      <td>1.333185</td>\n",
              "      <td>-0.493515</td>\n",
              "      <td>1.327774</td>\n",
              "      <td>-0.522781</td>\n",
              "      <td>1.333393</td>\n",
              "      <td>-0.592603</td>\n",
              "      <td>1.326325</td>\n",
              "      <td>0.250666</td>\n",
              "      <td>1.334906</td>\n",
              "      <td>-0.478272</td>\n",
              "      <td>1.324595</td>\n",
              "      <td>-0.469732</td>\n",
              "      <td>1.335614</td>\n",
              "      <td>-0.369642</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362398</th>\n",
              "      <td>1.328480</td>\n",
              "      <td>-0.668256</td>\n",
              "      <td>1.329517</td>\n",
              "      <td>-0.354503</td>\n",
              "      <td>1.327892</td>\n",
              "      <td>-0.795514</td>\n",
              "      <td>1.330085</td>\n",
              "      <td>-0.611748</td>\n",
              "      <td>1.328302</td>\n",
              "      <td>-0.870542</td>\n",
              "      <td>1.330707</td>\n",
              "      <td>-0.795843</td>\n",
              "      <td>1.330779</td>\n",
              "      <td>-0.890641</td>\n",
              "      <td>1.331192</td>\n",
              "      <td>-0.606696</td>\n",
              "      <td>1.330349</td>\n",
              "      <td>-0.979701</td>\n",
              "      <td>1.331664</td>\n",
              "      <td>-0.729342</td>\n",
              "      <td>1.329733</td>\n",
              "      <td>-0.239996</td>\n",
              "      <td>1.331329</td>\n",
              "      <td>-0.742665</td>\n",
              "      <td>1.328899</td>\n",
              "      <td>-0.712057</td>\n",
              "      <td>1.331219</td>\n",
              "      <td>-0.707649</td>\n",
              "      <td>1.327774</td>\n",
              "      <td>-0.522781</td>\n",
              "      <td>1.332409</td>\n",
              "      <td>-0.557970</td>\n",
              "      <td>1.326325</td>\n",
              "      <td>0.250666</td>\n",
              "      <td>1.332938</td>\n",
              "      <td>-0.188135</td>\n",
              "      <td>1.324595</td>\n",
              "      <td>-0.469732</td>\n",
              "      <td>1.333645</td>\n",
              "      <td>-0.433049</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362399</th>\n",
              "      <td>1.328480</td>\n",
              "      <td>-0.668256</td>\n",
              "      <td>1.329517</td>\n",
              "      <td>-0.354503</td>\n",
              "      <td>1.327892</td>\n",
              "      <td>-0.795514</td>\n",
              "      <td>1.330085</td>\n",
              "      <td>-0.611748</td>\n",
              "      <td>1.329282</td>\n",
              "      <td>-0.870542</td>\n",
              "      <td>1.330707</td>\n",
              "      <td>-0.795843</td>\n",
              "      <td>1.330779</td>\n",
              "      <td>-0.890641</td>\n",
              "      <td>1.331192</td>\n",
              "      <td>-0.606696</td>\n",
              "      <td>1.330349</td>\n",
              "      <td>-0.792900</td>\n",
              "      <td>1.331664</td>\n",
              "      <td>-0.729342</td>\n",
              "      <td>1.329733</td>\n",
              "      <td>-0.239996</td>\n",
              "      <td>1.331329</td>\n",
              "      <td>-0.742665</td>\n",
              "      <td>1.328899</td>\n",
              "      <td>-0.712057</td>\n",
              "      <td>1.331219</td>\n",
              "      <td>-0.707649</td>\n",
              "      <td>1.327774</td>\n",
              "      <td>-0.522781</td>\n",
              "      <td>1.332409</td>\n",
              "      <td>-0.557970</td>\n",
              "      <td>1.326325</td>\n",
              "      <td>0.250666</td>\n",
              "      <td>1.332938</td>\n",
              "      <td>-0.188135</td>\n",
              "      <td>1.324595</td>\n",
              "      <td>-0.469732</td>\n",
              "      <td>1.333645</td>\n",
              "      <td>-0.433049</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>362400 rows Ã— 45 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             Pa0       Va0       Pb0       Vb0  ...  l02  l03  l05  l10\n",
              "0       0.318116 -0.564619  0.313539 -0.551889  ...    1    1    1    1\n",
              "1       0.318116 -0.662079  0.313539 -0.551889  ...    1    1    1    1\n",
              "2       0.317136 -0.723163  0.313539 -0.551889  ...    2    1    1    1\n",
              "3       0.317136 -0.585895  0.313539 -0.551889  ...    1    2    1    1\n",
              "4       0.317136 -0.585895  0.313539 -0.551889  ...    0    0    1    1\n",
              "...          ...       ...       ...       ...  ...  ...  ...  ...  ...\n",
              "362395  1.328480 -0.608544  1.329517 -0.550603  ...    0    0    0    0\n",
              "362396  1.328480 -0.608544  1.329517 -0.550603  ...    1    0    0    0\n",
              "362397  1.328480 -0.608544  1.329517 -0.550603  ...    1    1    0    0\n",
              "362398  1.328480 -0.668256  1.329517 -0.354503  ...    1    1    0    0\n",
              "362399  1.328480 -0.668256  1.329517 -0.354503  ...    1    1    1    0\n",
              "\n",
              "[362400 rows x 45 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOrPhSy7330W"
      },
      "source": [
        ""
      ],
      "id": "rOrPhSy7330W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0U2zEiRPsyP",
        "outputId": "2d960bae-28c2-4092-b6e0-31a6f7326a90"
      },
      "source": [
        "target_name = 'l03'\n",
        "\n",
        "target_class_proportion  = test_data[target_name].value_counts()\n",
        "target_class_proportion /= target_class_proportion.sum()\n",
        "target_class_proportion"
      ],
      "id": "w0U2zEiRPsyP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    0.504994\n",
              "0    0.258634\n",
              "2    0.236372\n",
              "Name: l03, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiJUeaMQ_ZzL"
      },
      "source": [
        "train_gen = get_ts_generator(train_data, target_name=target_name)\n",
        "test_gen  = get_ts_generator(test_data,  target_name=target_name)"
      ],
      "id": "aiJUeaMQ_ZzL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVUCfm4ROXjI"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "        self.rate = rate\n",
        "        self.epsilon = 1e-6\n",
        "\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=self.epsilon)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=self.epsilon)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'embed_dim': self.embed_dim,\n",
        "            'num_heads': self.num_heads,\n",
        "            'ff_dim': self.ff_dim,\n",
        "            'rate': self.rate,\n",
        "        })\n",
        "        return config"
      ],
      "id": "gVUCfm4ROXjI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Sf2rjdN_ZzL"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "    \n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def get_model(num_features: int = 40, num_lstm_cells: int = 64) -> Model:\n",
        "    input_lmd = Input(shape=(LOOKUP_WINDOW_SIZE, num_features, 1))\n",
        "\n",
        "    # build the convolutional block\n",
        "    conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(input_lmd)\n",
        "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
        "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
        "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
        "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
        "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
        "\n",
        "    conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(conv_first1)\n",
        "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
        "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
        "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
        "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
        "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
        "\n",
        "    conv_first1 = Conv2D(32, (1, 10))(conv_first1)\n",
        "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
        "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
        "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
        "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
        "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
        "\n",
        "    # build the inception module\n",
        "    convsecond_1 = Conv2D(64, (1, 1), padding='same')(conv_first1)\n",
        "    convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)\n",
        "    convsecond_1 = Conv2D(64, (3, 1), padding='same')(convsecond_1)\n",
        "    convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)\n",
        "\n",
        "    convsecond_2 = Conv2D(64, (1, 1), padding='same')(conv_first1)\n",
        "    convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)\n",
        "    convsecond_2 = Conv2D(64, (5, 1), padding='same')(convsecond_2)\n",
        "    convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)\n",
        "\n",
        "    convsecond_3 = MaxPooling2D((3, 1), strides=(1, 1), padding='same')(conv_first1)\n",
        "    convsecond_3 = Conv2D(64, (1, 1), padding='same')(convsecond_3)\n",
        "    convsecond_3 = LeakyReLU(alpha=0.01)(convsecond_3)\n",
        "\n",
        "    convsecond_output = concatenate([convsecond_1, convsecond_2, convsecond_3], axis=3)\n",
        "    conv_reshape = Reshape((int(convsecond_output.shape[1]), int(convsecond_output.shape[3])))(convsecond_output)\n",
        "\n",
        "    # build the last LSTM layer\n",
        "    #conv_lstm = LSTM(num_lstm_cells)(conv_reshape)\n",
        "    transf = TransformerBlock(192,1,32)(conv_reshape)\n",
        "    x = layers.GlobalAveragePooling1D()(transf)\n",
        "    x = layers.Dropout(0.1)(x)\n",
        "    x = layers.Dense(32, activation=\"relu\")(x) \n",
        "    x = layers.Dropout(0.1)(x)\n",
        "    # build the output layer\n",
        "    out = Dense(3, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=input_lmd, outputs=out)\n",
        "    adam  = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1)\n",
        "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy', f1_m])\n",
        "\n",
        "    return model"
      ],
      "id": "5Sf2rjdN_ZzL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6R5N_Rr_ZzL",
        "outputId": "737b1ce1-17de-4b73-f3ed-1c0dd5b13fef"
      },
      "source": [
        "model = get_model()\n",
        "model.summary()"
      ],
      "id": "A6R5N_Rr_ZzL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            [(None, 100, 40, 1)] 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, 100, 20, 32)  96          input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_98 (LeakyReLU)      (None, 100, 20, 32)  0           conv2d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, 100, 20, 32)  4128        leaky_re_lu_98[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_99 (LeakyReLU)      (None, 100, 20, 32)  0           conv2d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_100 (Conv2D)             (None, 100, 20, 32)  4128        leaky_re_lu_99[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_100 (LeakyReLU)     (None, 100, 20, 32)  0           conv2d_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_101 (Conv2D)             (None, 100, 10, 32)  2080        leaky_re_lu_100[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_101 (LeakyReLU)     (None, 100, 10, 32)  0           conv2d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_102 (Conv2D)             (None, 100, 10, 32)  4128        leaky_re_lu_101[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_102 (LeakyReLU)     (None, 100, 10, 32)  0           conv2d_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_103 (Conv2D)             (None, 100, 10, 32)  4128        leaky_re_lu_102[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_103 (LeakyReLU)     (None, 100, 10, 32)  0           conv2d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_104 (Conv2D)             (None, 100, 1, 32)   10272       leaky_re_lu_103[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_104 (LeakyReLU)     (None, 100, 1, 32)   0           conv2d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_105 (Conv2D)             (None, 100, 1, 32)   4128        leaky_re_lu_104[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_105 (LeakyReLU)     (None, 100, 1, 32)   0           conv2d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_106 (Conv2D)             (None, 100, 1, 32)   4128        leaky_re_lu_105[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_106 (LeakyReLU)     (None, 100, 1, 32)   0           conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_107 (Conv2D)             (None, 100, 1, 64)   2112        leaky_re_lu_106[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_109 (Conv2D)             (None, 100, 1, 64)   2112        leaky_re_lu_106[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_107 (LeakyReLU)     (None, 100, 1, 64)   0           conv2d_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_109 (LeakyReLU)     (None, 100, 1, 64)   0           conv2d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 100, 1, 32)   0           leaky_re_lu_106[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_108 (Conv2D)             (None, 100, 1, 64)   12352       leaky_re_lu_107[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_110 (Conv2D)             (None, 100, 1, 64)   20544       leaky_re_lu_109[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_111 (Conv2D)             (None, 100, 1, 64)   2112        max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_108 (LeakyReLU)     (None, 100, 1, 64)   0           conv2d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_110 (LeakyReLU)     (None, 100, 1, 64)   0           conv2d_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_111 (LeakyReLU)     (None, 100, 1, 64)   0           conv2d_111[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 100, 1, 192)  0           leaky_re_lu_108[0][0]            \n",
            "                                                                 leaky_re_lu_110[0][0]            \n",
            "                                                                 leaky_re_lu_111[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "reshape_7 (Reshape)             (None, 100, 192)     0           concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "transformer_block_7 (Transforme (None, 100, 192)     161504      reshape_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_7 (Glo (None, 192)          0           transformer_block_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dropout_30 (Dropout)            (None, 192)          0           global_average_pooling1d_7[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_30 (Dense)                (None, 32)           6176        dropout_30[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_31 (Dropout)            (None, 32)           0           dense_30[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_31 (Dense)                (None, 3)            99          dropout_31[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 244,227\n",
            "Trainable params: 244,227\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfke25eTm7Bf"
      },
      "source": [
        "#tf.keras.utils.plot_model(model, to_file='trans.png', show_shapes=False, show_layer_names = False)"
      ],
      "id": "cfke25eTm7Bf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAj9Tr3J_ZzM",
        "outputId": "be3599d5-5520-4a41-a048-38bf61fac09a"
      },
      "source": [
        "model_saver = ModelCheckpoint(\n",
        "    model_path,\n",
        "    monitor='val_loss',\n",
        "    mode='auto',\n",
        "    verbose=0,\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    epochs=100,\n",
        "    verbose=1,\n",
        "    validation_data=test_gen,\n",
        "    callbacks=[model_saver, tensorboard_callback]\n",
        ")"
      ],
      "id": "tAj9Tr3J_ZzM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "5661/5661 [==============================] - 125s 22ms/step - loss: 1.0584 - accuracy: 0.4677 - f1_m: 0.2230 - val_loss: 1.0150 - val_accuracy: 0.5232 - val_f1_m: 0.4636\n",
            "Epoch 2/100\n",
            "5661/5661 [==============================] - 122s 21ms/step - loss: 1.0435 - accuracy: 0.4740 - f1_m: 0.2577 - val_loss: 1.0183 - val_accuracy: 0.5201 - val_f1_m: 0.4772\n",
            "Epoch 3/100\n",
            "5661/5661 [==============================] - 122s 21ms/step - loss: 1.0336 - accuracy: 0.4746 - f1_m: 0.2687 - val_loss: 1.0145 - val_accuracy: 0.5179 - val_f1_m: 0.4428\n",
            "Epoch 4/100\n",
            "5661/5661 [==============================] - 122s 22ms/step - loss: 1.0272 - accuracy: 0.4781 - f1_m: 0.2826 - val_loss: 1.0187 - val_accuracy: 0.5190 - val_f1_m: 0.4467\n",
            "Epoch 5/100\n",
            "5661/5661 [==============================] - 122s 21ms/step - loss: 1.0239 - accuracy: 0.4795 - f1_m: 0.2909 - val_loss: 1.0170 - val_accuracy: 0.5225 - val_f1_m: 0.4561\n",
            "Epoch 6/100\n",
            "5661/5661 [==============================] - 122s 21ms/step - loss: 1.0219 - accuracy: 0.4815 - f1_m: 0.3003 - val_loss: 1.0051 - val_accuracy: 0.5190 - val_f1_m: 0.4089\n",
            "Epoch 7/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 1.0191 - accuracy: 0.4811 - f1_m: 0.3030 - val_loss: 1.0019 - val_accuracy: 0.5238 - val_f1_m: 0.4356\n",
            "Epoch 8/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 1.0129 - accuracy: 0.4885 - f1_m: 0.3160 - val_loss: 1.0682 - val_accuracy: 0.5177 - val_f1_m: 0.4815\n",
            "Epoch 9/100\n",
            "5661/5661 [==============================] - 122s 21ms/step - loss: 1.0075 - accuracy: 0.4912 - f1_m: 0.3216 - val_loss: 0.9682 - val_accuracy: 0.5580 - val_f1_m: 0.4044\n",
            "Epoch 10/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.9870 - accuracy: 0.5069 - f1_m: 0.3540 - val_loss: 0.9354 - val_accuracy: 0.5755 - val_f1_m: 0.5201\n",
            "Epoch 11/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.9431 - accuracy: 0.5339 - f1_m: 0.4112 - val_loss: 0.8717 - val_accuracy: 0.5941 - val_f1_m: 0.4819\n",
            "Epoch 12/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.9080 - accuracy: 0.5565 - f1_m: 0.4394 - val_loss: 0.8529 - val_accuracy: 0.6015 - val_f1_m: 0.5284\n",
            "Epoch 13/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8851 - accuracy: 0.5656 - f1_m: 0.4552 - val_loss: 0.8560 - val_accuracy: 0.6003 - val_f1_m: 0.5417\n",
            "Epoch 14/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8724 - accuracy: 0.5731 - f1_m: 0.4652 - val_loss: 0.8228 - val_accuracy: 0.6149 - val_f1_m: 0.5214\n",
            "Epoch 15/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8581 - accuracy: 0.5793 - f1_m: 0.4707 - val_loss: 0.8865 - val_accuracy: 0.5976 - val_f1_m: 0.5535\n",
            "Epoch 16/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8528 - accuracy: 0.5820 - f1_m: 0.4753 - val_loss: 0.8531 - val_accuracy: 0.6031 - val_f1_m: 0.5477\n",
            "Epoch 17/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8446 - accuracy: 0.5854 - f1_m: 0.4805 - val_loss: 0.8272 - val_accuracy: 0.6084 - val_f1_m: 0.5475\n",
            "Epoch 18/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8418 - accuracy: 0.5866 - f1_m: 0.4824 - val_loss: 0.8076 - val_accuracy: 0.6132 - val_f1_m: 0.5445\n",
            "Epoch 19/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8409 - accuracy: 0.5869 - f1_m: 0.4897 - val_loss: 0.8146 - val_accuracy: 0.6090 - val_f1_m: 0.5557\n",
            "Epoch 20/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8336 - accuracy: 0.5908 - f1_m: 0.4982 - val_loss: 0.8215 - val_accuracy: 0.6086 - val_f1_m: 0.5525\n",
            "Epoch 21/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8331 - accuracy: 0.5922 - f1_m: 0.5000 - val_loss: 0.8214 - val_accuracy: 0.6130 - val_f1_m: 0.5547\n",
            "Epoch 22/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8293 - accuracy: 0.5930 - f1_m: 0.5020 - val_loss: 0.8075 - val_accuracy: 0.6205 - val_f1_m: 0.5245\n",
            "Epoch 23/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8297 - accuracy: 0.5927 - f1_m: 0.5027 - val_loss: 0.8067 - val_accuracy: 0.6170 - val_f1_m: 0.5657\n",
            "Epoch 24/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8278 - accuracy: 0.5936 - f1_m: 0.5082 - val_loss: 0.8041 - val_accuracy: 0.6184 - val_f1_m: 0.5479\n",
            "Epoch 25/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8234 - accuracy: 0.5959 - f1_m: 0.5086 - val_loss: 0.7991 - val_accuracy: 0.6220 - val_f1_m: 0.5556\n",
            "Epoch 26/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8252 - accuracy: 0.5950 - f1_m: 0.5101 - val_loss: 0.7987 - val_accuracy: 0.6232 - val_f1_m: 0.5687\n",
            "Epoch 27/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8218 - accuracy: 0.5993 - f1_m: 0.5168 - val_loss: 0.8046 - val_accuracy: 0.6211 - val_f1_m: 0.5567\n",
            "Epoch 28/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8228 - accuracy: 0.5959 - f1_m: 0.5152 - val_loss: 0.7959 - val_accuracy: 0.6257 - val_f1_m: 0.5597\n",
            "Epoch 29/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8206 - accuracy: 0.5964 - f1_m: 0.5177 - val_loss: 0.8025 - val_accuracy: 0.6245 - val_f1_m: 0.5692\n",
            "Epoch 30/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8210 - accuracy: 0.5962 - f1_m: 0.5204 - val_loss: 0.8023 - val_accuracy: 0.6242 - val_f1_m: 0.5452\n",
            "Epoch 31/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8198 - accuracy: 0.5980 - f1_m: 0.5219 - val_loss: 0.7943 - val_accuracy: 0.6262 - val_f1_m: 0.5737\n",
            "Epoch 32/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8185 - accuracy: 0.5993 - f1_m: 0.5244 - val_loss: 0.7943 - val_accuracy: 0.6226 - val_f1_m: 0.5695\n",
            "Epoch 33/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8177 - accuracy: 0.6007 - f1_m: 0.5238 - val_loss: 0.7972 - val_accuracy: 0.6205 - val_f1_m: 0.5784\n",
            "Epoch 34/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8191 - accuracy: 0.5988 - f1_m: 0.5205 - val_loss: 0.7959 - val_accuracy: 0.6236 - val_f1_m: 0.5792\n",
            "Epoch 35/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8150 - accuracy: 0.5995 - f1_m: 0.5267 - val_loss: 0.7967 - val_accuracy: 0.6219 - val_f1_m: 0.5755\n",
            "Epoch 36/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8185 - accuracy: 0.5983 - f1_m: 0.5267 - val_loss: 0.7908 - val_accuracy: 0.6267 - val_f1_m: 0.5617\n",
            "Epoch 37/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8168 - accuracy: 0.5989 - f1_m: 0.5216 - val_loss: 0.7972 - val_accuracy: 0.6237 - val_f1_m: 0.5482\n",
            "Epoch 38/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8151 - accuracy: 0.6002 - f1_m: 0.5230 - val_loss: 0.7910 - val_accuracy: 0.6305 - val_f1_m: 0.5602\n",
            "Epoch 39/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8144 - accuracy: 0.5997 - f1_m: 0.5239 - val_loss: 0.7882 - val_accuracy: 0.6327 - val_f1_m: 0.5803\n",
            "Epoch 40/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8139 - accuracy: 0.6009 - f1_m: 0.5284 - val_loss: 0.7990 - val_accuracy: 0.6258 - val_f1_m: 0.5631\n",
            "Epoch 41/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8133 - accuracy: 0.6012 - f1_m: 0.5302 - val_loss: 0.7959 - val_accuracy: 0.6244 - val_f1_m: 0.5703\n",
            "Epoch 42/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8128 - accuracy: 0.6020 - f1_m: 0.5285 - val_loss: 0.7952 - val_accuracy: 0.6225 - val_f1_m: 0.5755\n",
            "Epoch 43/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8128 - accuracy: 0.6007 - f1_m: 0.5283 - val_loss: 0.8000 - val_accuracy: 0.6234 - val_f1_m: 0.5701\n",
            "Epoch 44/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8117 - accuracy: 0.6019 - f1_m: 0.5309 - val_loss: 0.7904 - val_accuracy: 0.6291 - val_f1_m: 0.5565\n",
            "Epoch 45/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8129 - accuracy: 0.6002 - f1_m: 0.5263 - val_loss: 0.7953 - val_accuracy: 0.6258 - val_f1_m: 0.5682\n",
            "Epoch 46/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8075 - accuracy: 0.6050 - f1_m: 0.5347 - val_loss: 0.8024 - val_accuracy: 0.6279 - val_f1_m: 0.5740\n",
            "Epoch 47/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8100 - accuracy: 0.6042 - f1_m: 0.5318 - val_loss: 0.8083 - val_accuracy: 0.6246 - val_f1_m: 0.5804\n",
            "Epoch 48/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8116 - accuracy: 0.6026 - f1_m: 0.5348 - val_loss: 0.8089 - val_accuracy: 0.6212 - val_f1_m: 0.5804\n",
            "Epoch 49/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8108 - accuracy: 0.6023 - f1_m: 0.5298 - val_loss: 0.8253 - val_accuracy: 0.6216 - val_f1_m: 0.5618\n",
            "Epoch 50/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8103 - accuracy: 0.6031 - f1_m: 0.5312 - val_loss: 0.7934 - val_accuracy: 0.6296 - val_f1_m: 0.5833\n",
            "Epoch 51/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8087 - accuracy: 0.6049 - f1_m: 0.5339 - val_loss: 0.7966 - val_accuracy: 0.6299 - val_f1_m: 0.5822\n",
            "Epoch 52/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8096 - accuracy: 0.6027 - f1_m: 0.5262 - val_loss: 0.7989 - val_accuracy: 0.6211 - val_f1_m: 0.5795\n",
            "Epoch 53/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8070 - accuracy: 0.6044 - f1_m: 0.5349 - val_loss: 0.7939 - val_accuracy: 0.6233 - val_f1_m: 0.5812\n",
            "Epoch 54/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8070 - accuracy: 0.6034 - f1_m: 0.5316 - val_loss: 0.7979 - val_accuracy: 0.6303 - val_f1_m: 0.5662\n",
            "Epoch 55/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8086 - accuracy: 0.6047 - f1_m: 0.5337 - val_loss: 0.7975 - val_accuracy: 0.6257 - val_f1_m: 0.5809\n",
            "Epoch 56/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8080 - accuracy: 0.6030 - f1_m: 0.5357 - val_loss: 0.7907 - val_accuracy: 0.6309 - val_f1_m: 0.5825\n",
            "Epoch 57/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8058 - accuracy: 0.6064 - f1_m: 0.5396 - val_loss: 0.7955 - val_accuracy: 0.6284 - val_f1_m: 0.5819\n",
            "Epoch 58/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8065 - accuracy: 0.6062 - f1_m: 0.5368 - val_loss: 0.7916 - val_accuracy: 0.6290 - val_f1_m: 0.5855\n",
            "Epoch 59/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8043 - accuracy: 0.6081 - f1_m: 0.5392 - val_loss: 0.7893 - val_accuracy: 0.6331 - val_f1_m: 0.5796\n",
            "Epoch 60/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8066 - accuracy: 0.6072 - f1_m: 0.5397 - val_loss: 0.7940 - val_accuracy: 0.6262 - val_f1_m: 0.5839\n",
            "Epoch 61/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8063 - accuracy: 0.6072 - f1_m: 0.5414 - val_loss: 0.7883 - val_accuracy: 0.6301 - val_f1_m: 0.5922\n",
            "Epoch 62/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8079 - accuracy: 0.6063 - f1_m: 0.5433 - val_loss: 0.7940 - val_accuracy: 0.6224 - val_f1_m: 0.5787\n",
            "Epoch 63/100\n",
            "5661/5661 [==============================] - 122s 21ms/step - loss: 0.8048 - accuracy: 0.6067 - f1_m: 0.5435 - val_loss: 0.7974 - val_accuracy: 0.6282 - val_f1_m: 0.5867\n",
            "Epoch 64/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8036 - accuracy: 0.6097 - f1_m: 0.5464 - val_loss: 0.7889 - val_accuracy: 0.6323 - val_f1_m: 0.5877\n",
            "Epoch 65/100\n",
            "5661/5661 [==============================] - 121s 21ms/step - loss: 0.8064 - accuracy: 0.6085 - f1_m: 0.5436 - val_loss: 0.7936 - val_accuracy: 0.6327 - val_f1_m: 0.5893\n",
            "Epoch 66/100\n",
            "5661/5661 [==============================] - 122s 21ms/step - loss: 0.8010 - accuracy: 0.6120 - f1_m: 0.5511 - val_loss: 0.7985 - val_accuracy: 0.6326 - val_f1_m: 0.5896\n",
            "Epoch 67/100\n",
            "5661/5661 [==============================] - 122s 21ms/step - loss: 0.8016 - accuracy: 0.6114 - f1_m: 0.5481 - val_loss: 0.7860 - val_accuracy: 0.6396 - val_f1_m: 0.5903\n",
            "Epoch 68/100\n",
            "5661/5661 [==============================] - 122s 21ms/step - loss: 0.8038 - accuracy: 0.6112 - f1_m: 0.5497 - val_loss: 0.7841 - val_accuracy: 0.6441 - val_f1_m: 0.5897\n",
            "Epoch 69/100\n",
            "5661/5661 [==============================] - 122s 21ms/step - loss: 0.8005 - accuracy: 0.6155 - f1_m: 0.5529 - val_loss: 0.7916 - val_accuracy: 0.6400 - val_f1_m: 0.5912\n",
            "Epoch 70/100\n",
            "5661/5661 [==============================] - 122s 21ms/step - loss: 0.8039 - accuracy: 0.6218 - f1_m: 0.5523 - val_loss: 0.7926 - val_accuracy: 0.6504 - val_f1_m: 0.5922\n",
            "Epoch 71/100\n",
            "5661/5661 [==============================] - 122s 21ms/step - loss: 0.7974 - accuracy: 0.6315 - f1_m: 0.5573 - val_loss: 0.7823 - val_accuracy: 0.6593 - val_f1_m: 0.5950\n",
            "Epoch 72/100\n",
            "5661/5661 [==============================] - 122s 21ms/step - loss: 0.7922 - accuracy: 0.6401 - f1_m: 0.5777 - val_loss: 0.7695 - val_accuracy: 0.6692 - val_f1_m: 0.6353\n",
            "Epoch 73/100\n",
            "5661/5661 [==============================] - 122s 21ms/step - loss: 0.7890 - accuracy: 0.6472 - f1_m: 0.5973 - val_loss: 0.7644 - val_accuracy: 0.6750 - val_f1_m: 0.6460\n",
            "Epoch 74/100\n",
            "5661/5661 [==============================] - 122s 22ms/step - loss: 0.7861 - accuracy: 0.6530 - f1_m: 0.6138 - val_loss: 0.7772 - val_accuracy: 0.6753 - val_f1_m: 0.6491\n",
            "Epoch 75/100\n",
            "5661/5661 [==============================] - 122s 22ms/step - loss: 0.7834 - accuracy: 0.6555 - f1_m: 0.6203 - val_loss: 0.7634 - val_accuracy: 0.6819 - val_f1_m: 0.6543\n",
            "Epoch 76/100\n",
            "5661/5661 [==============================] - 122s 22ms/step - loss: 0.7798 - accuracy: 0.6569 - f1_m: 0.6239 - val_loss: 0.7673 - val_accuracy: 0.6772 - val_f1_m: 0.6590\n",
            "Epoch 77/100\n",
            "5661/5661 [==============================] - 122s 22ms/step - loss: 0.7772 - accuracy: 0.6602 - f1_m: 0.6298 - val_loss: 0.7567 - val_accuracy: 0.6832 - val_f1_m: 0.6670\n",
            "Epoch 78/100\n",
            "5661/5661 [==============================] - 122s 22ms/step - loss: 0.7747 - accuracy: 0.6631 - f1_m: 0.6335 - val_loss: 0.7558 - val_accuracy: 0.6910 - val_f1_m: 0.6719\n",
            "Epoch 79/100\n",
            "5661/5661 [==============================] - 122s 22ms/step - loss: 0.7694 - accuracy: 0.6664 - f1_m: 0.6376 - val_loss: 0.7474 - val_accuracy: 0.6963 - val_f1_m: 0.6729\n",
            "Epoch 80/100\n",
            "5661/5661 [==============================] - 122s 22ms/step - loss: 0.7702 - accuracy: 0.6671 - f1_m: 0.6387 - val_loss: 0.7494 - val_accuracy: 0.6937 - val_f1_m: 0.6734\n",
            "Epoch 81/100\n",
            "5661/5661 [==============================] - 122s 22ms/step - loss: 0.7643 - accuracy: 0.6724 - f1_m: 0.6462 - val_loss: 0.7501 - val_accuracy: 0.6960 - val_f1_m: 0.6782\n",
            "Epoch 82/100\n",
            "5661/5661 [==============================] - 122s 22ms/step - loss: 0.7626 - accuracy: 0.6723 - f1_m: 0.6457 - val_loss: 0.7483 - val_accuracy: 0.6947 - val_f1_m: 0.6768\n",
            "Epoch 83/100\n",
            "5661/5661 [==============================] - 122s 22ms/step - loss: 0.7612 - accuracy: 0.6754 - f1_m: 0.6506 - val_loss: 0.7419 - val_accuracy: 0.6962 - val_f1_m: 0.6839\n",
            "Epoch 84/100\n",
            "5661/5661 [==============================] - 122s 22ms/step - loss: 0.7601 - accuracy: 0.6755 - f1_m: 0.6499 - val_loss: 0.7374 - val_accuracy: 0.7008 - val_f1_m: 0.6883\n",
            "Epoch 85/100\n",
            "5661/5661 [==============================] - 122s 22ms/step - loss: 0.7595 - accuracy: 0.6757 - f1_m: 0.6509 - val_loss: 0.7440 - val_accuracy: 0.6975 - val_f1_m: 0.6865\n",
            "Epoch 86/100\n",
            "5661/5661 [==============================] - 122s 22ms/step - loss: 0.7557 - accuracy: 0.6785 - f1_m: 0.6555 - val_loss: 0.7353 - val_accuracy: 0.7046 - val_f1_m: 0.6878\n",
            "Epoch 87/100\n",
            "5661/5661 [==============================] - 122s 22ms/step - loss: 0.7566 - accuracy: 0.6778 - f1_m: 0.6542 - val_loss: 0.7392 - val_accuracy: 0.7006 - val_f1_m: 0.6899\n",
            "Epoch 88/100\n",
            "5661/5661 [==============================] - 122s 22ms/step - loss: 0.7531 - accuracy: 0.6813 - f1_m: 0.6584 - val_loss: 0.7307 - val_accuracy: 0.7051 - val_f1_m: 0.6892\n",
            "Epoch 89/100\n",
            "5661/5661 [==============================] - 122s 22ms/step - loss: 0.7485 - accuracy: 0.6831 - f1_m: 0.6605 - val_loss: 0.7369 - val_accuracy: 0.7000 - val_f1_m: 0.6868\n",
            "Epoch 90/100\n",
            "5661/5661 [==============================] - 122s 22ms/step - loss: 0.7491 - accuracy: 0.6827 - f1_m: 0.6600 - val_loss: 0.7400 - val_accuracy: 0.7010 - val_f1_m: 0.6855\n",
            "Epoch 91/100\n",
            "5661/5661 [==============================] - 122s 22ms/step - loss: 0.7485 - accuracy: 0.6837 - f1_m: 0.6629 - val_loss: 0.7383 - val_accuracy: 0.6980 - val_f1_m: 0.6883\n",
            "Epoch 92/100\n",
            "5661/5661 [==============================] - 122s 22ms/step - loss: 0.7458 - accuracy: 0.6859 - f1_m: 0.6644 - val_loss: 0.7274 - val_accuracy: 0.7045 - val_f1_m: 0.6967\n",
            "Epoch 93/100\n",
            "5661/5661 [==============================] - 122s 22ms/step - loss: 0.7433 - accuracy: 0.6874 - f1_m: 0.6668 - val_loss: 0.7389 - val_accuracy: 0.7040 - val_f1_m: 0.6946\n",
            "Epoch 94/100\n",
            "5661/5661 [==============================] - 122s 22ms/step - loss: 0.7430 - accuracy: 0.6895 - f1_m: 0.6679 - val_loss: 0.7375 - val_accuracy: 0.7062 - val_f1_m: 0.6926\n",
            "Epoch 95/100\n",
            "5661/5661 [==============================] - 122s 22ms/step - loss: 0.7431 - accuracy: 0.6894 - f1_m: 0.6688 - val_loss: 0.7376 - val_accuracy: 0.7064 - val_f1_m: 0.6950\n",
            "Epoch 96/100\n",
            "5661/5661 [==============================] - 122s 22ms/step - loss: 0.7381 - accuracy: 0.6921 - f1_m: 0.6729 - val_loss: 0.7252 - val_accuracy: 0.7072 - val_f1_m: 0.6949\n",
            "Epoch 97/100\n",
            "5661/5661 [==============================] - 122s 22ms/step - loss: 0.7393 - accuracy: 0.6909 - f1_m: 0.6714 - val_loss: 0.7369 - val_accuracy: 0.7048 - val_f1_m: 0.6932\n",
            "Epoch 98/100\n",
            "5661/5661 [==============================] - 122s 22ms/step - loss: 0.7401 - accuracy: 0.6912 - f1_m: 0.6721 - val_loss: 0.7257 - val_accuracy: 0.7102 - val_f1_m: 0.6951\n",
            "Epoch 99/100\n",
            "5661/5661 [==============================] - 122s 22ms/step - loss: 0.7381 - accuracy: 0.6917 - f1_m: 0.6721 - val_loss: 0.7241 - val_accuracy: 0.7135 - val_f1_m: 0.7052\n",
            "Epoch 100/100\n",
            "5661/5661 [==============================] - 122s 22ms/step - loss: 0.7374 - accuracy: 0.6930 - f1_m: 0.6747 - val_loss: 0.7274 - val_accuracy: 0.7082 - val_f1_m: 0.6992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "nD9UIK3olRYc",
        "outputId": "4f59b3d3-dd93-4fdb-bb19-f95c0063d958"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "id": "nD9UIK3olRYc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}